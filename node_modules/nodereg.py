# To add a new cell, type ''
# To add a new markdown cell, type '#%%[markdown]'

#%%[markdown]
#
# # HW Regression
# ## By:Kismt Khatri
# ### Date: 28/11/2022
#
# 
# We have the historic Titanic dataset to study here. You can use `Titanic` with rfit.dfapi
# to load the dataset, or if given, from local file `Titanic.csv`.  
# The variables in the dataset are:  

# * `survival`: Survival,	0 = No, 1 = Yes
# * `pclass`: Ticket class, 1 = 1st, 2 = 2nd, 3 = 3rd
# * `sex`: Gender / Sex
# * `age`: Age in years
# * `sibsp`: # of siblings / spouses on the Titanic
# * `parch`: # of parents / children on the Titanic
# * `ticket`: Ticket number (for superstitious ones)
# * `fare`: Passenger fare
# * `embarked`: Port of Embarkment	C: Cherbourg, Q: Queenstown, S: Southampton

#%%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import rfit
#%%

# Question 1
# Load the Titanic dataset
titanic = rfit.dfapi('Titanic','id')
# then perform some summary statistics. 
# a)	Histogram on age. Maybe a stacked histogram on age with male-female as two series if possible
# b)	proportion summary of male-female, survived-dead
# c)	pie chart for “Ticket class”
# d)	A single visualization chart that shows info of survival, age, pclass, and sex.
# 
# a
titanic.dropna()
titanic.pivot(columns='sex').age.plot(kind = 'hist', stacked=True)
plt.title("Stacked histogram on age with sex")
plt.xlabel("Age")
plt.show()

#b
crosstab = pd.crosstab(titanic['survived'], titanic['sex'], margins = True)
print(crosstab)

#c
Ticketclass = titanic['pclass'].value_counts()
fig, ax = plt.subplots()
ax.pie(Ticketclass, labels=Ticketclass.index, autopct='%1.1f%%')
plt.title("pie chart for Ticketclass")
plt.legend()
plt.show()

#d
sexcolors = np.where(titanic['pclass']==1,'r','-') 
sexcolors[titanic['pclass']==2] = 'b'
sexcolors[titanic['pclass']==3] = 'g'

ax1 = titanic[titanic.sex=='male'].plot(x="age", y="survived", kind="scatter", color=sexcolors[titanic.sex=='male'], marker='^', s=7, label='male')
titanic[titanic.sex=='female'].plot(x="age", y="survived", kind="scatter", color=sexcolors[titanic.sex=='female'], marker='+', s=7, label='female', ax = ax1)
plt.title("Age vs Survival")
plt.show()


#%%
fig, ax = plt.subplots(figsize=(15,8), nrows=1, ncols=3)
plt.suptitle('Age Distribution of Survivors and Non-Survivors split by Gender in Each Class of tickets', fontsize=16)

for i in range(3):
    ax[i].set_title('Class {}'.format(i+1))
    ax[i].set_ylim(-5,85)
    sns.swarmplot(data=titanic[titanic['pclass']==i+1],
                  x='survived',
                  y='age',
                  hue='sex',
                  hue_order=['male','female'],
                  size=3,
                  ax=ax[i])

ax[1].set_ylabel(None)
ax[2].set_ylabel(None)

ax[0].legend_.remove()
ax[1].legend_.remove()
# %%
# Question 2
# Using the statsmodels package, 
# build a logistic regression model for survival. Include the features that you find plausible. 
# Make sure categorical variables are use properly. If the coefficient(s) turns out insignificant, drop it and re-build.
# 
# 
import statsmodels.api as sm
from statsmodels.formula.api import glm
Logit = glm(formula='survived~C(pclass)+age+C(sex)+sibsp+parch', data=titanic, family=sm.families.Binomial())
LogitFit = Logit.fit()
print(LogitFit.summary())

# When alpha = 0.05, the p-value of "parch" is 0.840. Thus we fail to reject the null hypothesis that the coefficient of "parch" is 0. 
Logit_re = glm(formula='survived~C(pclass)+age+C(sex)+sibsp', data=titanic, family=sm.families.Binomial())
LogitFit_re = Logit_re.fit()
print(LogitFit_re.summary())
#%% 
# Question 3

# Interpret your result. 
# What are the factors and how do they affect the chance of survival (or the survival odds ratio)? 
# What is the predicted probability of survival for a 30-year-old female with a second class ticket, 
# no siblings, 3 parents/children on the trip? Use whatever variables that are relevant in your model.
# 
# 
# 
# Using pclass =1 as the base line, when the pclass =2, the odds ratio of survived becomes exp(-0.9510) times. When the pclass =3, the odds ratio of survived becomes exp(-2.1575) times.
# Using female as the base line, the odds ratio of male survived becomes exp(-2.7393) times.
# 1 degree increase in age makes odds ratio of survived exp(-0.0181) times as before while other variables stay the same. 
# 1 degree increase in age makes odds ratio of survived exp(-0.2804) times as before while other variables stay the same. 
pred = pd.DataFrame({'pclass':2, 'sex':"female", 'age':30, 'sibsp':0, 'parch':3}, pd.Index(range(1)))
print("The predicted probability of survival for a 30-year-old female with a second class ticket, no siblings, 3 parents/children on the trip is %.2f" % LogitFit_re.predict(pred))
#%%
# Question 4
# Now use the sklearn package, perform the same model and analysis as in Question 3. 
# In sklearn however, it is easy to set up the train-test split before we build the model. 
# Use 67-33 split to solve this problem. 
# Find out the accuracy score of the model.
# 
# 
# 
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

titanic['sex'][titanic['sex']=='male'] = 0
titanic['sex'][titanic['sex']=='female'] = 1

xsurvival = titanic[['pclass', 'age', 'sex', 'sibsp']]
ysurvival = titanic['survived']


x_trainSurvival, x_testSurvival, y_trainSurvival, y_testSurvival = train_test_split(xsurvival, ysurvival, test_size=0.33, random_state=1 )


survivallogit = LogisticRegression()
survivallogit.fit(x_trainSurvival, y_trainSurvival)

predictions_survived = survivallogit.predict(x_testSurvival)
print("Accuracy score of the model:", survivallogit.score(x_testSurvival,y_testSurvival))


#%%
# Question 5
# Try three different cut-off values at 0.3, 0.5, and 0.7. What are the 
# a)	Total accuracy of the model
# b)	The precision of the model for 0 and for 1
# c)	The recall rate of the model for 0 and for 1
# 
# 
def predictcutoff(arr, cutoff):
  arrbool = arr[:,1]>cutoff
  arr= arr[:,1]*arrbool/arr[:,1]
  return arr.astype(int)

test = survivallogit.predict_proba(x_testSurvival)
y_pred3 = predictcutoff(test, 0.3)
print(classification_report(y_testSurvival, y_pred3))
#  The accuracy of the model is 0.78. The precision for "0" is 0.86, for "1" is 0.70. The recall rate for "0" is 0.73, for "1" is 0.84.

y_pred5 = predictcutoff(test, 0.5)
print(classification_report(y_testSurvival, y_pred5))
#  The accuracy of the model is 0.79. The precision for "0" is 0.79, for "1" is 0.80. The recall rate for "0" is 0.88, for "1" is 0.68.

y_pred7 = predictcutoff(test, 0.7)
print(classification_report(y_testSurvival, y_pred7))
#  The accuracy of the model is 0.75. The precision for "0" is 0.70, for "1" is 0.95. The recall rate for "0" is 0.98, for "1" is 0.43.

#%% 
# Question 6
# By using cross-validation, re-do the logit regression, and evaluate 
# the 10-fold average accuracy of the logit model. 
# Use the same predictors you had from previous questions.
#
#
from sklearn.model_selection import cross_val_score
cross_validation = cross_val_score(survivallogit,xsurvival,ysurvival, cv=10 )
print("Probilities:" , cross_validation)
print("All probabilities average", np.mean(cross_validation))


